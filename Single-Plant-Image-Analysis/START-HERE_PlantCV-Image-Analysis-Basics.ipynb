{"cells":[{"cell_type":"markdown","id":"innocent-insured","metadata":{"tags":[],"id":"innocent-insured"},"source":["<h1 style=\"font-size:4rem;color:orange;\">What Does an Image Look Like in Python? </h1>"]},{"cell_type":"markdown","id":"adverse-spank","metadata":{"id":"adverse-spank"},"source":["Together with your partner, advance through this self-guided activity to learn more about how Python and PlantCV convert images into data."]},{"cell_type":"markdown","id":"natural-clothing","metadata":{"tags":[],"id":"natural-clothing"},"source":["# What makes up images?\n","Digital images are made up of a mosaic of really small squares called ***pixels***. You can think of them similar to how ancient art was made using mosaic tiles like the image below:\n","<p style=\"text-align:center;\"><img src=\"https://www.worldhistory.org/uploads/images/12743.jpg?v=1661607482\" width=\"400\" height=\"500\" />\n"]},{"cell_type":"markdown","id":"efficient-maldives","metadata":{"id":"efficient-maldives"},"source":["The arrangement of these tiles are fashioned in a way that it give the appearance of smooth lines that our eyes can follow. If the square tiles, or ***pixels***, are too large, then it is much hard to make smooth edges and curves like 8-bit Mario.\n","![Mario Timeline](https://yumyummatt.files.wordpress.com/2011/02/super-mario-timeline1.jpg)"]},{"cell_type":"markdown","id":"regulated-agency","metadata":{"id":"regulated-agency"},"source":["\"These small little dots [*pixels*] are what make up the images on computer displays, whether they are flat-screen (LCD) or tube (CRT) monitors. The screen is divided up into a matrix of thousands or even millions of pixels.Typically, you cannot see the individual pixels, because they are so small. This is a good thing, because most people prefer to look at smooth, clear images rather than blocky, \"pixelated\" ones.\n","\n","However, if you set your monitor to a low resolution, such as 640x480 and look closely at your screen, you will may be able to see the individual pixels.\n","As you may have guessed, a resolution of 640x480 is comprised of a matrix of 640 by 480 pixels, or 307,200 in all. That's a lot of little dots.\"\n","\n","<p style=\"text-align:center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f2/Resolution_illustration.png\" />"]},{"cell_type":"markdown","id":"noble-posting","metadata":{"tags":[],"id":"noble-posting"},"source":["# Images and Matrices\n","There are many ways we can store pixels, or the **picture elements**, of digital images. Take a look at the image of Felix the Cat below. We can represent this image with a 35 x 35 matrix using binary unites, or ***bits***. These numbers will tell us the color of each pixel in the image, where 0 represents black and 1 indicates white.\n","<p style=\"text-align:center;\"><img src= \"attachment:21bc9862-72a8-48c5-ad50-0964b017beac.png\" />\n","\n","So we see that these units exist within a pixel coordinate system that resembles a grid or matrix with **rows** and **columns** that each hold pixel values that represent a color on the digital image, with the origin in the top left corner and the coordinate values increase going down and right.\n","<p style=\"text-align:center;\"><img src=\"https://iq.opengenus.org/content/images/2020/04/index.png\" />"]},{"cell_type":"markdown","id":"signal-disclosure","metadata":{"id":"signal-disclosure"},"source":["Furthermore, there are 8-bits in a ***byte***, which is the fundamental unit of storage on computer. Since each *bit* contains one of two values and 8 bits make a byte, we find that there are 256 possible permutations of binary code that can represent each *pixel*. Binary images like the one above are just one way we can store pixel values. Each element in the matrix determines the intensity of the corresponding pixel.\n","\n","However, when we look at grayscale images we see that the colors are not binary but instead exist within a gradient. With 256 possible bytes available, we see that the values in each coordinate become more discrete. We see that 0 still represents black (lowest intensity), but now white is represented by 255 (maximum intensity).\n","\n","In Python, we can use image analyses packages to store our images as arrays using the NumPy package. Using NumPy we can determine the shape and size of the image, where the dimensions are ordered y (rows), x (columns, and z (channels) for images."]},{"cell_type":"markdown","id":"excess-lesbian","metadata":{"id":"excess-lesbian"},"source":["# Color (RGB) Images"]},{"cell_type":"code","execution_count":null,"id":"warming-jersey","metadata":{"id":"warming-jersey"},"outputs":[],"source":["from IPython.display import Audio, Video, YouTubeVideo\n","id='l8_fZPHasdo'\n","YouTubeVideo(id=id, width=\"600\", height=\"300\")"]},{"cell_type":"markdown","id":"latter-series","metadata":{"id":"latter-series"},"source":["* **To submit code for execution, click on the cell to make it active, then hold SHIFT and press Enter**"]},{"cell_type":"markdown","id":"varying-imperial","metadata":{"id":"varying-imperial"},"source":["This is a color image encoded using a Red Green Blue (RGB) color model.\n","Note: that in OpenCV the color order is BGR.\n","\n","<img src=\"https://i.stack.imgur.com/buuZr.png\" />\n","\n","The color channels are stacked in a third dimension and increase from the 'front' to the 'back'\n","\n","How is color broadcasted?\n","* It can be emitted from a black background (computer monitor).\n","\n","* It can be reflected when deposited from a color ink jet onto white paper."]},{"cell_type":"markdown","id":"primary-nature","metadata":{"id":"primary-nature"},"source":["## What is a color space?\n","Color information can be represented using different models, or [color spaces](https://en.wikipedia.org/wiki/Color_space). The *color space* describes the type of information used to produce representations of a color."]},{"cell_type":"code","execution_count":null,"id":"exempt-pledge","metadata":{"id":"exempt-pledge"},"outputs":[],"source":["#What is the color of the grass outside? Print your answer.\n"]},{"cell_type":"markdown","id":"addressed-carolina","metadata":{"id":"addressed-carolina"},"source":["Think about your response, what kind of information can you gather from that?\n","* Hue of the color (Green)\n","* Saturation of the color (how light or dark is the grass?)"]},{"cell_type":"markdown","id":"vital-palestinian","metadata":{"id":"vital-palestinian"},"source":["The color we visualize is a combination of multiple colors but depending on the color space they could be a combination of Red/Green/Blue (RGB) or L* a* b* (CIELAB)\n","\n","Let's take a look at TV and computer monitors. If you look very closely at your computer monitor (I DO NOT RECOMMEND DOING THIS), you may be able to see small patterns of circles that are combinations of little red, green, and blue lights. The intensity of these lights facilitate the display of the images on our monitor. How these colors mix together defines how \"crisp\" of a color and image we see.\n","\n","Typical color images use an additive [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model):\n","<p style=\"text-align:center;\"><img src=\"https://cdn.britannica.com/89/234589-050-0E6E9D4B/color-wheel-additive.jpg\" width=\"400\" height=\"400\" />\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Pixel_geometry_01_Pengo.jpg/480px-Pixel_geometry_01_Pengo.jpg\" width=\"500\" height=\"400\" />"]},{"cell_type":"markdown","id":"about-snake","metadata":{"id":"about-snake"},"source":["Next, we will consider the [CIELAB colorspace](https://en.wikipedia.org/wiki/CIELAB_color_space). Also referred to as the L* a* b* colorspace, this colorspace represents colors as ac ombination of lightness, A, and B components. Lightness represents the perceived brightness of the color, where A and B represent the green-red/magenta and blue-yellow color components, respectively. This colorspace is designed to approximate human vision better than the other colorspaces.\n","\n","<p style=\"text-align:center;\"><img src=\"https://cms-assets.tutsplus.com/cdn-cgi/image/width=1700/uploads/users/346/posts/25468/image/ColorLab_D60Secfinal.jpg\" />\n","<p style=\"text-align:center;\"><img src=\"https://rkcolor.com/wp-content/uploads/2023/03/CIELAB-Color-spce-3D.jpg\" />"]},{"cell_type":"markdown","id":"realistic-stomach","metadata":{"tags":[],"id":"realistic-stomach"},"source":["Let's play with our Jupyter environment to create some color images.\n","\n","First, we need to load our libraries."]},{"cell_type":"markdown","id":"e0caebed-5d13-45d1-824a-b88f9916c617","metadata":{"id":"e0caebed-5d13-45d1-824a-b88f9916c617"},"source":["# Example Workflow"]},{"cell_type":"markdown","id":"df5cab6d-ab0e-4e5d-85ad-f0c1eab52022","metadata":{"id":"df5cab6d-ab0e-4e5d-85ad-f0c1eab52022"},"source":["## Loading Libraries"]},{"cell_type":"code","source":["%pip install \"altair>=5\" ipympl  plantcv\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"WBsA8wCgMpwF","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1692850619996,"user_tz":300,"elapsed":117782,"user":{"displayName":"Parag Bhatt","userId":"11827230773587162498"}},"outputId":"c5911533-4ebc-4ca0-d64f-3938006255b2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting altair>=5\n","  Downloading altair-5.0.1-py3-none-any.whl (471 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipympl\n","  Downloading ipympl-0.9.3-py2.py3-none-any.whl (511 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting plantcv\n","  Downloading plantcv-4.0-py3-none-any.whl (327 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair>=5) (3.1.2)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=5) (4.19.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair>=5) (1.23.5)\n","Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from altair>=5) (1.5.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=5) (0.12.0)\n","Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from altair>=5) (4.7.1)\n","Requirement already satisfied: ipython<9 in /usr/local/lib/python3.10/dist-packages (from ipympl) (7.34.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipympl) (0.2.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from ipympl) (9.4.0)\n","Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.10/dist-packages (from ipympl) (5.7.1)\n","Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from ipympl) (7.7.1)\n","Requirement already satisfied: matplotlib<4,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from ipympl) (3.7.1)\n","Collecting matplotlib<4,>=3.4.0 (from ipympl)\n","  Downloading matplotlib-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy (from altair>=5)\n","  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from plantcv) (2.8.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from plantcv) (1.10.1)\n","Requirement already satisfied: scikit-image>=0.19 in /usr/local/lib/python3.10/dist-packages (from plantcv) (0.19.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from plantcv) (1.2.2)\n","Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from plantcv) (2023.8.0)\n","Collecting dask-jobqueue (from plantcv)\n","  Downloading dask_jobqueue-0.8.2-py2.py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from plantcv) (4.8.0.76)\n","Requirement already satisfied: xarray>=2022.11.0 in /usr/local/lib/python3.10/dist-packages (from plantcv) (2023.7.0)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from plantcv) (0.14.0)\n","Collecting vl-convert-python (from plantcv)\n","  Downloading vl_convert_python-0.12.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (67.7.2)\n","Collecting jedi>=0.16 (from ipython<9->ipympl)\n","  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl) (4.8.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (5.5.6)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.6.5)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.8)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5) (0.9.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (4.42.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (23.1)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (3.1.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair>=5) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->plantcv) (1.16.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19->plantcv) (3.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19->plantcv) (2.31.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19->plantcv) (2023.8.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19->plantcv) (1.4.1)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv) (8.1.7)\n","Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv) (2.2.1)\n","Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv) (2023.6.0)\n","Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv) (1.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv) (6.0.1)\n","Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv) (6.8.0)\n","Requirement already satisfied: distributed>=2022.02.0 in /usr/local/lib/python3.10/dist-packages (from dask-jobqueue->plantcv) (2023.8.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair>=5) (2.1.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->plantcv) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->plantcv) (3.2.0)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->plantcv) (0.5.3)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv) (1.0.0)\n","Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv) (1.0.5)\n","Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv) (5.9.5)\n","Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv) (2.0.0)\n","Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv) (6.3.2)\n","Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv) (2.0.4)\n","Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv) (3.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->plantcv) (3.16.2)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.1.12)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<9->ipympl) (0.2.6)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.5.5)\n","Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (23.2.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (23.1.0)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.3.1)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.9.2)\n","Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.5.4)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.5.7)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.2)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.17.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.17.1)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.0.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.10.0)\n","Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.24.0)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.2.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.9.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.11.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.0.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.8.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.5.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.2.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.18.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (21.2.0)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.6.1)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.15.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.4.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.1.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.21)\n","Installing collected packages: vl-convert-python, numpy, jedi, matplotlib, dask-jobqueue, altair, plantcv, ipympl\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.7.1\n","    Uninstalling matplotlib-3.7.1:\n","      Successfully uninstalled matplotlib-3.7.1\n","  Attempting uninstall: altair\n","    Found existing installation: altair 4.2.2\n","    Uninstalling altair-4.2.2:\n","      Successfully uninstalled altair-4.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","plotnine 0.12.2 requires matplotlib>=3.6.0, but you have matplotlib 3.5.3 which is incompatible.\n","plotnine 0.12.2 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed altair-5.0.1 dask-jobqueue-0.8.2 ipympl-0.9.3 jedi-0.19.0 matplotlib-3.5.3 numpy-1.22.4 plantcv-4.0 vl-convert-python-0.12.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits","numpy"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"id":"WBsA8wCgMpwF"},{"cell_type":"code","execution_count":1,"id":"acute-algeria","metadata":{"tags":[],"id":"acute-algeria","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692850643880,"user_tz":300,"elapsed":7400,"user":{"displayName":"Parag Bhatt","userId":"11827230773587162498"}},"outputId":"0593ed5e-6286-4e16-88bc-644877137d9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1LV2620sF2ER0ciOg917LLdTzmTK5D3EY/20230828-UFMG-PlantCV-Workshop/Image-Analysis-using-PlantCV/Single-Plant-Analysis\n","Athaliana.png\n","Athaliana_result\n","PlantCV-Image-Analysis-Basics-reference.html\n","PlantCV-Image-Analysis-Basics-reference.ipynb\n","START-HERE_PlantCV-Image-Analysis-Basics.html\n","START-HERE_PlantCV-Image-Analysis-Basics.ipynb\n"]}],"source":["# Matplotlib enables us to plot within the notebook, matplotlib is very powerful plotting library\n","%matplotlib inline\n","# Imports NumPy package into notebook, essential for scientific computing\n","import numpy as np\n","# Imports PlantCV into notebook so that we can conduct plant phenotyping analyses\n","from plantcv import plantcv as pcv\n","# Imports library to handle workflow inputs compatible with parallel workflow execution.\n","from plantcv.parallel import WorkflowInputs\n","# Imports PyPlot which will provides us a MATLAB-like interface\n","from matplotlib import pyplot as plt\n","# Change working directory to point to Google Drive folder where notebook is stored.\n","%cd '/content/gdrive/MyDrive/Colab Notebooks/20230828-UFMG-PlantCV-Workshop/Image-Analysis-using-PlantCV/Single-Plant-Analysis'\n","!ls"]},{"cell_type":"code","execution_count":null,"id":"2e873da9-2704-45bc-bba0-e8c9daecc815","metadata":{"tags":[],"id":"2e873da9-2704-45bc-bba0-e8c9daecc815"},"outputs":[],"source":["# Input/output options\n","args = WorkflowInputs(\n","    images=[\"\"],\n","    names=\"\",\n","    result=\"\",\n","    outdir=\".\",\n","    writeimg=True,\n","    debug=\"plot\",\n","    )"]},{"cell_type":"markdown","id":"competent-increase","metadata":{"id":"competent-increase"},"source":["**For more information on the class Params, check out**\n","https://plantcv.readthedocs.io/en/4.x/params/"]},{"cell_type":"markdown","id":"ed00be53-b2b3-4dcf-8c44-f02d6bf4c607","metadata":{"id":"ed00be53-b2b3-4dcf-8c44-f02d6bf4c607"},"source":["## Setting Your PlantCV Environment Parameters"]},{"cell_type":"code","execution_count":null,"id":"impossible-vertex","metadata":{"id":"impossible-vertex"},"outputs":[],"source":["# Set debug to the global parameter\n","pcv.params.debug = args.debug\n","# Change display settings\n","pcv.params.dpi = 100\n","# Now we set up our parameters for our PlantCV environment\n","pcv.params.text_size=20\n","pcv.params.text_thickness=20"]},{"cell_type":"markdown","id":"023e9402-686b-47f3-b722-59d9caf6b209","metadata":{"id":"023e9402-686b-47f3-b722-59d9caf6b209"},"source":["## Sample Plot"]},{"cell_type":"code","execution_count":null,"id":"popular-honduras","metadata":{"id":"popular-honduras"},"outputs":[],"source":["# Let's create a plot space that is filled with pixel values of zero (true black)\n","# We will make a matrix that is 250 rows by 250 columns with 3 channels\n","my_rgb_img = np.zeros((), dtype=np.uint8)\n","pcv.plot_image(my_rgb_img)"]},{"cell_type":"markdown","id":"opening-woman","metadata":{"id":"opening-woman"},"source":["**You should see a black box that contains 250 rows and columns, filled with pixel values of [0,0,0]**"]},{"cell_type":"code","execution_count":null,"id":"everyday-gnome","metadata":{"id":"everyday-gnome"},"outputs":[],"source":["#Now let's make a color image:\n","#We can give values to each color channel by defining the pixel in each RGB channel individually\n","#[:,:,0] -> Indexes the Blue channel\n","#[:,:,1] -> Indexes the Green channel\n","#[:,:,2] -> Indexes the Red channel\n","\n","#Run the cell below to see how we can \"turn on\" certain pixels by indexing our NumPy array.\n","#We set the values of the subsetted data to their maximum value (255)\n","my_rgb_img[:150,:150,0] = 255\n","my_rgb_img[50:200,50:200,1] = 255\n","my_rgb_img[100:250,100:250,2] = 255\n","pcv.plot_image(my_rgb_img)"]},{"cell_type":"markdown","id":"mobile-female","metadata":{"id":"mobile-female"},"source":["Notice how only a few of our pixels are \"turned on\" fully. By influencing the individual channels (the 3rd element in the tuple) and assigning those channels to pixel intensities between 0 and 255, we are able to alter the values of pixels discretely. Observe how in the region where all three channels are set to the max value (255) the color is white\n","\n","Tinker with some of these values and see if you can create a mosaic of colors where different colors are represented within my_rgb_img.\n","\n","Now, let's start working with some color images. We are going to begin by reading in a sample *A. thaliana* image."]},{"cell_type":"markdown","id":"de9641da-2d77-402a-9811-fff8bf07258d","metadata":{"id":"de9641da-2d77-402a-9811-fff8bf07258d"},"source":["## Reading Images into PlantCV\n","Inputs:\n","* filename = name of image file\n","* mode     = mode of imread (\"native\", \"rgb\", \"rgba\", \"gray\", \"csv\", \"envi\", \"arcgis\").\n","             **Default is \"native.\"**\n","\n","Returns:\n","* img      = image object as numpy array\n","* path     = path to image file\n","* img_name = name of image file"]},{"cell_type":"code","execution_count":null,"id":"virtual-module","metadata":{"id":"virtual-module"},"outputs":[],"source":["# Read in the sample image of Arabidopsis into PlantCV.\n","# Leave **mode** as Default.\n","img, path, img_name = pcv.readimage(filename=\n","                                   )"]},{"cell_type":"markdown","id":"70482262-575c-44d5-bbcf-e4cf0ca242f4","metadata":{"id":"70482262-575c-44d5-bbcf-e4cf0ca242f4"},"source":["## Investigating Your Image"]},{"cell_type":"code","execution_count":null,"id":"chronic-sewing","metadata":{"id":"chronic-sewing"},"outputs":[],"source":["# Determine the shape and size of our RGB image below:\n","#The output will tell us (# of rows, # of columns, # of color channels)\n","img.shape"]},{"cell_type":"code","execution_count":null,"id":"protected-coach","metadata":{"id":"protected-coach"},"outputs":[],"source":["#Determine the data type of the image below\n","img.dtype"]},{"cell_type":"code","execution_count":null,"id":"final-medium","metadata":{"id":"final-medium"},"outputs":[],"source":["#Identify the minimum pixel value found in the image between all three channels\n","np.min(img)"]},{"cell_type":"code","execution_count":null,"id":"stable-wayne","metadata":{"id":"stable-wayne"},"outputs":[],"source":["#Identify the maximum pixel value found in our image between all three channels\n","np.max(img)"]},{"cell_type":"code","execution_count":null,"id":"social-multimedia","metadata":{"tags":[],"id":"social-multimedia"},"outputs":[],"source":["#We described earlier that each color channel is its own matrix\n","#To show this, we will pull out the green channel and it will show each is a grayscale channel with its own respective data\n","#We will use pcv.plot_image(img=img[:,:,1]) to extract the green channel data\n","#[:,:,0] -> Blue channel\n","#[:,:,1] -> Green channel\n","#[:,:,2] -> Red channel\n","pcv.plot_image(img=img[:,:,1])"]},{"cell_type":"markdown","id":"noble-divorce","metadata":{"id":"noble-divorce"},"source":["**What is happening in the other channels?**\n","* Change the 3rd element to 0 or 2 to see what the Blue and Red channels."]},{"cell_type":"code","execution_count":null,"id":"collect-marker","metadata":{"id":"collect-marker"},"outputs":[],"source":["#Calculate the min, max, and mean values from the green channel to see how many pixels are represented in that channel\n","#We can check out the \"pixel stats\" for each channel to see where the most pixel intensity exists.\n","print(np.min(img[:,:,1]))\n","print(np.max(img[:,:,1]))\n","print(np.mean(img[:,:,1]))"]},{"cell_type":"markdown","id":"e05e7110-dac4-4259-979a-1ee15cd51595","metadata":{"id":"e05e7110-dac4-4259-979a-1ee15cd51595"},"source":["## Investigating Colorspaces"]},{"cell_type":"markdown","id":"controversial-enzyme","metadata":{"id":"controversial-enzyme"},"source":["For image analysis and visual perception of color properties, other color models such as [Hue, Saturation, and Value (HSV)](https://en.wikipedia.org/wiki/HSL_and_HSV) or [CIELAB (LAB)](https://en.wikipedia.org/wiki/CIELAB_color_space) have advantages over RGB.\n","\n","In the next exercise we are going to visualize the color spaces available in PlantCV so we can label the plant material and distinguish the plant from the background.\n","We will use the function *pcv.visualize.colorspaces()* to help us see our image (img) in the various color models (HSV and LAB). We need to store the color space plot in a variable so just name the new variable *cs*. The last thing we will do is set original_img to **False** as we don't need to see our original image, we only care about the color space.\n","\n","As you start typing out the method, be sure to use the TAB key to autocomplete the method so you don't end up with typographical errors. Once you complete typing *pcv.visualize.colorspaces()*, press SHIFT + TAB to view the helper to see how to set up the method.\n"]},{"cell_type":"code","execution_count":null,"id":"initial-class","metadata":{"id":"initial-class"},"outputs":[],"source":["# Visualize component HSV and LAB color spaces\n","cs = pcv.visualize.colorspaces(rgb_img=,\n","                               original_img=\n","                              )"]},{"cell_type":"markdown","id":"basic-conviction","metadata":{"id":"basic-conviction"},"source":["Let's take a moment to understand what each of the color models above are showing us. We will start with describing what HSV color spaces can tell us:\n","<p style=\"text-align:center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Hsl-hsv_models.svg/240px-Hsl-hsv_models.svg.png\" />\n","\n","* Hue - refers to the color of the pixel, the absolute representation of the color\n","* Saturation - refers to how \"colorful\" the pixel is (i.e., the difference between light green and forest green)\n","* Value - refers to how \"white\" the pixel is (0-255)\n","\n","<p style=\"text-align:center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Visible_gamut_within_CIELAB_color_space_D65_whitepoint_mesh.png/240px-Visible_gamut_within_CIELAB_color_space_D65_whitepoint_mesh.png\" />\n","\n","* Lightness - similar to the Value color space, how much \"white\" or \"brightness\" is in the color\n","* A - This color space describes the green/red-magenta values\n","* B - This color spaces describes the blue/yellow values\n","\n","We are visualizing our plant material in these spaces to maximize the differences between the \"plant material\" and the \"background\".\n","In our image, these differences will be more easily observed. When we use use top view images, looking at our images in these color spaces are very important as they will help us discern \"plant material\" from \"soil media\""]},{"cell_type":"code","execution_count":null,"id":"focused-ceiling","metadata":{"id":"focused-ceiling"},"outputs":[],"source":["print(input(\"Looking above at the 6 color spaces produced, which color space produces the greatest contrast between plant and background?\"))"]},{"cell_type":"markdown","id":"appointed-expert","metadata":{"id":"appointed-expert"},"source":["Use this color space that you determined to have the greatest contrast between plant and background, we are now going to convert our RGB image and cast it into grayscale but in the channel you chose.\n","\n","(If you are unsure how to set up the method, press SHIFT + TAB to access the helper)"]},{"cell_type":"markdown","id":"44e649ac-9494-40b9-8b88-1111c5a49e65","metadata":{"id":"44e649ac-9494-40b9-8b88-1111c5a49e65"},"source":["## Convert Image to Grayscale"]},{"cell_type":"code","execution_count":null,"id":"parallel-secretariat","metadata":{"tags":[],"id":"parallel-secretariat"},"outputs":[],"source":["# The output of this method will be stored in the variable named *gray_img* since we are\n","# producing a grayscale image.\n","# The functions we will be using are *pcv.rgb2gray_hsv()* or *pcv.rgb2gray_lab()*\n","# Set the rgb_img to our *img* and define the channel with the color space you thought had\n","# the greatest contrast.\n","\n","# Convert the RGB image into a grayscale image by choosing one of the HSV or LAB channels\n","gray_img = pcv.rgb2gray_lab(rgb_img=,\n","                            channel=\"\"\n","                           )"]},{"cell_type":"markdown","id":"1b3be04c-bcf8-469d-bb29-2790c5f4e0af","metadata":{"id":"1b3be04c-bcf8-469d-bb29-2790c5f4e0af"},"source":["Now that we have our grayscale image, let's see which pixels refer to our plant and which are the background"]},{"cell_type":"markdown","id":"eb74cd03-f776-460f-aaaa-f0ee353dcd3a","metadata":{"id":"eb74cd03-f776-460f-aaaa-f0ee353dcd3a"},"source":["## Visualizing pixel distribution in an image"]},{"cell_type":"markdown","id":"dated-infrared","metadata":{"id":"dated-infrared"},"source":["Inputs:\n","* img            = an RGB or grayscale image to analyze\n","* mask           = binary mask, calculate histogram from masked area only (default=None)\n","* bins           = divide the data into n evenly spaced bins (default=100)\n","* lower_bound    = the lower bound of the bins (x-axis min value) (default=None)\n","* upper_bound    = the upper bound of the bins (x-axis max value) (default=None)\n","* title          = a custom title for the plot (default=None)\n","* hist_data      = return the frequency distribution data if True (default=False)\n","\n","Returns:\n","* fig_hist       = histogram figure\n","* hist_df        = dataframe with histogram data, with columns \"pixel intensity\" and \"proportion                    of pixels (%)\"\n","*Look at your picture, what percentage of the picture is plant versus background? It will be helpful to keep this in mind when you look at the histogram output.*"]},{"cell_type":"code","execution_count":null,"id":"visible-design","metadata":{"tags":[],"id":"visible-design"},"outputs":[],"source":["# Use *pcv.visualize.histogram()* to see the distribution of pixel values in the grayscale\n","# and store it in the variable *hist*.\n","# We will create **50 bins** to start with but play with the bins until you can see\n","# discrete peaks.\n","# Set *img* to *gray_img* since that is the image whose pixel distribution we wish to see.\n","\n","# Visualize a histogram of the grayscale values to identify signal related to the plant\n","# vs the background.\n","hist = pcv.visualize.histogram(img=,\n","                               bins=\n","                              )"]},{"cell_type":"markdown","id":"level-april","metadata":{"id":"level-april"},"source":["What does your chart look like? Compare your histogram with your neighbor's and see where the peaks are on the chart. As a refresher, histograms aggregate information into discrete bins that satisfy a range of values. The more values that fall within that \"bin\", the larger the peak on the chart."]},{"cell_type":"code","execution_count":null,"id":"unexpected-strap","metadata":{"id":"unexpected-strap"},"outputs":[],"source":["print(input(\"Where is the largest peak located on the histogram?\"))"]},{"cell_type":"code","execution_count":null,"id":"surprised-syndicate","metadata":{"id":"surprised-syndicate"},"outputs":[],"source":["print(input(\"What part of our picture do you think is the big peak, is it the plant or the background?\"))"]},{"cell_type":"markdown","id":"particular-slovak","metadata":{"id":"particular-slovak"},"source":["*Discuss amongst yourselves what the other peaks on the histogram are referring to in our image and change the number of bins in the histogram to see how the pixel intensities are sorted. How did your finds compare with what you just found out?*"]},{"cell_type":"markdown","id":"29854953-f2b6-4aa8-b251-9caff93ce698","metadata":{"id":"29854953-f2b6-4aa8-b251-9caff93ce698"},"source":["# Creating a Mask"]},{"cell_type":"markdown","id":"b039bdad-a763-4948-9801-357f67801d89","metadata":{"id":"b039bdad-a763-4948-9801-357f67801d89"},"source":["## Binary Thresholding"]},{"cell_type":"markdown","id":"alert-grant","metadata":{"id":"alert-grant"},"source":["The next step is to create a binary mask that excludes pixel data from the background, but shows the pixel intensities from the plant material. To do this we will use the function *pcv.threshold.binary()* to set a binary threshold that labels the plant pixels white and the background as black.\n","\n","Inputs:\n","* gray_img     = Grayscale image data\n","* threshold    = Threshold value (0-255)\n","* object_type  = \"light\" or \"dark\" (default: \"light\")\n","               - If object is lighter than the background then standard thresholding is done\n","               - If object is darker than the background then inverse thresholding is done\n","\n","Returns:\n","* bin_img      = Thresholded, binary image\n","\n","(Press SHIFT + TAB key to see the helper so we can set the inputs.)"]},{"cell_type":"code","execution_count":null,"id":"european-omega","metadata":{"id":"european-omega"},"outputs":[],"source":["# We are going to store the information from this binary threshold in the variable bin_img.\n","\n","# Use the histogram to set a binary threshold where the plant pixels will be labeled white\n","# and the background will be labeled black\n","man_bin_img = pcv.threshold.binary(gray_img=,\n","                                   threshold=,\n","                                   object_type=\"\"\n","                                  )"]},{"cell_type":"markdown","id":"directed-provider","metadata":{"id":"directed-provider"},"source":["Congratulations! You just set a manual threshold to exclude pixel data that you found does not represent the plant material.\n","\n","PlantCV has the ability to automatically threshold using the function *pcv.threshold.otsu()* - our first step into using machine learning approaches.\n","\n","(Use SHIFT + TAB to observe the helper and see how to set up this method, it isn't too much different than setting up a manual binary threshold.)"]},{"cell_type":"code","execution_count":null,"id":"african-november","metadata":{"id":"african-november"},"outputs":[],"source":["# Instead of setting a manual threshold, try an automatic threshold method such as Otsu\n","auto_bin_img = pcv.threshold.otsu(gray_img=,\n","                                  object_type=\"\"\n","                                 )"]},{"cell_type":"markdown","id":"bacterial-movie","metadata":{"id":"bacterial-movie"},"source":["You might notice that some of the pixels from the background and color card made it into our Otsu-thresholded image, disregard these as they will not affect our analyses due on the design of our PlantCV workflow. What has happened is that the algorithm that Otsu's thresholding technique determined a pixel range that excluded the majority of the background, pot, and color card except for a few pixels (due to shading/color gradients).\n","\n","Otsu's thresholding technique runs an algorithm with the following steps:\n","1. Process the input image\n","2. Obtain image histogram (distribution of pixels)\n","3. Compute the threshold value T\n","4. Replace image pixels into white in those regions, where saturation is greater than T and into the black in the opposite cases.\n","\n","For more information on Otsu's thresholing technique, visit https://learnopencv.com/otsu-thresholding-with-opencv/"]},{"cell_type":"markdown","id":"green-medline","metadata":{"id":"green-medline"},"source":["Next step is to create a region of interest (ROI) so that we can later quantify the phenotype of the plant. To do so, we will begin by creating a circular ROI using our original image as a reference. We will then overlay our binary mask so that PlantCV can pull the stats from it."]},{"cell_type":"markdown","id":"5d595106-518b-4dcc-a5de-e1c162228599","metadata":{"id":"5d595106-518b-4dcc-a5de-e1c162228599"},"source":["## Region of Interest"]},{"cell_type":"markdown","id":"welcome-checkout","metadata":{"id":"welcome-checkout"},"source":["We will use *pcv.roi.circle()* to superimpose a circle onto our image. Use SHIFT + TAB to open up the helper to see what inputs you will need.\n","\n","Here is some helpful information that we will need to set up the code:\n","Inputs:\n","* img           = An RGB or grayscale image to plot the ROI on in debug mode.\n","* x             = The x-coordinate of the center of the circle.\n","* y             = The y-coordinate of the center of the circle.\n","* r             = The radius of the circle.\n","\n","Outputs:\n","* roi_contour   = An ROI set of points (contour)."]},{"cell_type":"code","execution_count":null,"id":"capital-vacuum","metadata":{"tags":[],"id":"capital-vacuum"},"outputs":[],"source":["# Define a region of interest (ROI) where we expect to find a plant\n","# For this image, look at the image and try to determine where the origin\n","# of your ROI circle will be.\n","roi = pcv.roi.circle(img=,\n","                     x=,\n","                     y=,\n","                     r=\n","                    )"]},{"cell_type":"markdown","id":"mature-tablet","metadata":{"id":"mature-tablet"},"source":["Excellent work! Now we are going to use our binary mask image along with our ROI to create a filtered mask so that we can have PlantCV analyze the components of this plant and not the background or the color card.\n","\n","We need to look at our manually-thresholded and the Otsu-generated binary masks to determine which of these binary masks contain the most \"plant.\" This will be the binary mask that we use to further filter our mask for overlaying on our image for precise measurement.\n","\n","In the previous step, we created an ROI circle that encapsulated as much as of the plant as possible. Since we care about total plant area, we want the binary mask that will contain the most plant material (i.e., the Otsu-generated binary mask - *auto_bin_img*)."]},{"cell_type":"markdown","id":"8ccd1abf-1553-4754-b800-2ef4c2a01ff6","metadata":{"id":"8ccd1abf-1553-4754-b800-2ef4c2a01ff6"},"source":["## Fixing Your ROI to the Binary Mask"]},{"cell_type":"markdown","id":"tough-papua","metadata":{"id":"tough-papua"},"source":["We are going to use *pcv.roi.filter()* to refine our binary mask by selecting only pixels that are related to those pixels selected under the ROI. Recall that in both our manually- and Otsu-thresholded binary masks we had pixels from the chamber get extracted, using this function we can select only the relevant pixels in the plant. Let's investigate the requirements for this function with SHIFT + Tab:\n","\n","Inputs:\n","* mask           = binary thresholded image data to be filtered.\n","* roi            = region of interest, an instance of the Object class output from a roi function\n","* roi_type       = 'cutto', 'partial' (for partially inside, default), or 'largest' (keep only the largest contour)\n","\n","Returns:\n","* filtered_mask     = mask image.\n","\n","*Work together to input the arguments to generate the filtered mask, set the **roi_type** to **cutto***"]},{"cell_type":"code","execution_count":null,"id":"entertaining-auction","metadata":{"id":"entertaining-auction"},"outputs":[],"source":["filtered_mask = pcv.roi.filter(mask=,\n","                               roi=,\n","                               roi_type=\"\"\n","                              )"]},{"cell_type":"markdown","id":"naked-affiliation","metadata":{"id":"naked-affiliation"},"source":["Our mask looks great! Notice how we have eliminated all the pixels except those related to our plant, even those in the tip of the littlest leaf.\n","\n","Our next step will be to gather object size data to see just how connected our mask is (i.e., is the mask one polygon or many little polygons close by)."]},{"cell_type":"markdown","id":"5ecfcf17-d801-4db9-9691-a484789329a1","metadata":{"id":"5ecfcf17-d801-4db9-9691-a484789329a1"},"source":["## Gathering Object Data"]},{"cell_type":"markdown","id":"bba66420-0b2c-4809-913f-1aeb5eaf61e3","metadata":{"id":"bba66420-0b2c-4809-913f-1aeb5eaf61e3"},"source":["The function *pcv.visualize.obj_sizes()* will display all of the objects in the image using your binary mask and original image. Investigate the properties of this function and play around with the **num_objects** parameter to determine how many objects are in the image and the sizes of those objects.\n","Inputs:\n","* img          = RGB or grayscale image data\n","* mask         = Binary mask made from filtered binary mask\n","* num_objects  = Optional parameter to limit the number of objects that will get annotated\n","\n","Returns:\n","* plotting_img = Plotting image with objects labeled by area"]},{"cell_type":"code","execution_count":null,"id":"temporal-proxy","metadata":{"id":"temporal-proxy"},"outputs":[],"source":["# Adjust the plot parameters so we can read the numbers.\n","pcv.params.text_size = 3\n","pcv.params.text_thickness = 5\n","\n","# Set the number of objects to 1, our main object\n","sizes = pcv.visualize.obj_sizes(img=,\n","                                mask=,\n","                                num_objects=1\n","                               )"]},{"cell_type":"markdown","id":"together-former","metadata":{"id":"together-former"},"source":["Well, apparently our filtered_mask isn't in one piece and is actually made up of about 421 objects."]},{"cell_type":"markdown","id":"a07c2a74-65bb-4329-ad94-a95f7a6e79c9","metadata":{"id":"a07c2a74-65bb-4329-ad94-a95f7a6e79c9"},"source":["## Reducing Image Noise"]},{"cell_type":"markdown","id":"d77ca68c-9762-41b7-87c9-8face73181ce","metadata":{"id":"d77ca68c-9762-41b7-87c9-8face73181ce"},"source":["We can connect some of the objects together using *pcv.fill_holes()*. This function fills all of the holes and connects several objects together with white pixels.\n","\n","Inputs:\n","* bin_img      = Filtered binary image data\n","\n","Returns:\n","* filtered_img = image with objects filled"]},{"cell_type":"code","execution_count":null,"id":"collective-floating","metadata":{"id":"collective-floating"},"outputs":[],"source":["filled_mask = pcv.fill_holes(bin_img=)"]},{"cell_type":"markdown","id":"floating-expense","metadata":{"id":"floating-expense"},"source":["Look at the image with mask overlay we produced, does it contain the aspects of the plant that we care about (i.e., as much \"plant material\" as possible)? If so, good! If not, then we need to become more discrete with our binary mask or we can filter the salt/noise so we get as much plant material as we can within our contoured layer."]},{"cell_type":"markdown","id":"funded-madrid","metadata":{"id":"funded-madrid"},"source":["Let's see how our flood fill worked using *pcv.visualize.obj_sizes()* again. We may not be able to completely connect all of the pixels, but we can try."]},{"cell_type":"code","execution_count":null,"id":"sustainable-deadline","metadata":{"tags":[],"id":"sustainable-deadline"},"outputs":[],"source":["sizes = pcv.visualize.obj_sizes(img=,\n","                                mask=,\n","                                num_objects=1\n","                               )"]},{"cell_type":"markdown","id":"b1a52a4a-f33d-4063-a3a4-e1321a64f6f5","metadata":{"id":"b1a52a4a-f33d-4063-a3a4-e1321a64f6f5"},"source":["Looks as though there are still 47 more objects in this image, likely background.\n","\n","## Fill\n","We can use the Fill function to fill objects below a certain pixel size. This will help us create a single object for analysis.\n","\n","Inputs:\n","* bin_img      = Binary image data (use the\n","* size         = minimum object area size in pixels (integer)\n","\n","\n","Returns:\n","* filtered_img = image with objects filled"]},{"cell_type":"code","execution_count":null,"id":"256567af-e7a8-4c1e-853a-42dac458705f","metadata":{"tags":[],"id":"256567af-e7a8-4c1e-853a-42dac458705f"},"outputs":[],"source":["clean_mask = pcv.fill(bin_img=,\n","                      size=\n","                     )"]},{"cell_type":"code","execution_count":null,"id":"c191a626-4b53-4f34-a8ac-3a6633ecd4c6","metadata":{"tags":[],"id":"c191a626-4b53-4f34-a8ac-3a6633ecd4c6"},"outputs":[],"source":["sizes = pcv.visualize.obj_sizes(img=,\n","                                mask=,\n","                                num_objects=1\n","                               )"]},{"cell_type":"markdown","id":"271fc8a3-8bd9-45ff-ad48-d9f16b6ee9cb","metadata":{"id":"271fc8a3-8bd9-45ff-ad48-d9f16b6ee9cb"},"source":["## Create labeled mask\n","We want to extract traits from each bean replicate, so we need to create a mask that has unique pixel values for each identified object.\n","\n","Inputs:\n","* mask            = cleaned mask image\n","* rois            = (Optional) list of multiple ROIs (from roi.multi or roi.auto_grid)\n","* roi_type        = (Optional)''partial' (for partially inside, default), cutto'\n","                    (hard cut at boundary), 'largest' (keep only the largest contour)\n","\n","Returns:\n","* mask            = Labeled mask\n","* num_labels      = Number of labeled objects"]},{"cell_type":"code","execution_count":null,"id":"e2e063e8-241c-442f-99af-8676d1bfba5b","metadata":{"id":"e2e063e8-241c-442f-99af-8676d1bfba5b"},"outputs":[],"source":["labeled_mask, num = pcv.create_labels(mask=)\n"]},{"cell_type":"markdown","id":"47a033b6-5a3c-45a4-8a87-eb5bdfbbcf6f","metadata":{"id":"47a033b6-5a3c-45a4-8a87-eb5bdfbbcf6f"},"source":["# Gathering Information From Your Image"]},{"cell_type":"markdown","id":"complimentary-portfolio","metadata":{"id":"complimentary-portfolio"},"source":["We're almost there! Now we just need to analyze our contours and see what information we can pull out!\n","\n","## Extract size data from your sample using *pcv.analyze.size()*\n","Inputs:\n","* img          = RGB image data for plotting\n","* labeled_mask = Labeled mask of objects (32-bit).\n","* n_labels     = Total number expected individual objects (default = 1).\n","* label        = Optional label parameter, modifies the variable name of observations recorded (default = \"default\").\n","\n","Returns:\n","* analysis_image = Diagnostic image showing measurements."]},{"cell_type":"code","execution_count":null,"id":"modular-damage","metadata":{"tags":[],"id":"modular-damage"},"outputs":[],"source":["#shape_img = pcv.analyze.size(img=img, labeled_mask=labeled_mask, n_labels=num, label=\"plant1\")\n","shape_img = pcv.analyze.size(img=,\n","                             labeled_mask=,\n","                             n_labels=,\n","                             label=\"\"\n","                            )"]},{"cell_type":"markdown","id":"f43c5127-04d4-4154-8016-cf94f1f557f8","metadata":{"id":"f43c5127-04d4-4154-8016-cf94f1f557f8"},"source":["## Extract color traits from your sample using *pcv.analyze.color()*\n","\n","Inputs:\n","\n","* img           = RGB image for debugging\n","* labeled_mask  = Grayscale mask with unique pixel value per object of interest\n","* n_labels      = Total number expected individual objects (default = 1).\n","* colorspaces   = 'all', 'rgb', 'lab', or 'hsv' (default = 'hsv')\n","* label         = Modifies the variable name of observations recorded (default = \"default\").\n","\n","Returns:\n","* analysis_image  = histogram output\n"]},{"cell_type":"code","execution_count":null,"id":"handed-index","metadata":{"tags":[],"id":"handed-index"},"outputs":[],"source":["# Measure the color properties of the plant\n","pcv.params.debug = \"plot\"\n","color_hist = pcv.analyze.color(rgb_img=,\n","                               labeled_mask=,\n","                               n_labels=,\n","                               colorspaces=\"\",\n","                               label=\"\"\n","                              )"]},{"cell_type":"markdown","id":"grateful-comfort","metadata":{"id":"grateful-comfort"},"source":["#### Remember that the histogram data is going to tell us how many pixels of a particular value are in a colorspace. If we think about the various properties of plants (chloroplast density, photosynthetic efficiency, water content in leaves, etc.) and how they can be affect due to high heat environments, these measurements will be critical for our analysis."]},{"cell_type":"markdown","id":"annual-ribbon","metadata":{"tags":[],"id":"annual-ribbon"},"source":["## Saving results\n","Analyzing a single plant is just the inital step of developing a workflow capable of performing high-throughput phenotyping. You will test your workflow on an increasing subset of image data to ensure its accuracy.\n","* *A recommended subsetted dataset schedule for training your algorithm is 1 image > 5 images > 20 images > 50 images*.\n","* Once you have developed sufficient accuracy with your workflow, then you will be ready to prepare a script for parallelization of the entirety of your image dataset for analysis."]},{"cell_type":"code","execution_count":null,"id":"brazilian-palace","metadata":{"id":"brazilian-palace"},"outputs":[],"source":["# We will collect the data stored from *pcv.analyze.size()* and *pcv.analyze_color()* and save it as a Comma-Separated Values (CSV) files.\n","# The filename will be set to *plant1_result*.\n","pcv.outputs.save_results(,\n","                         outformat=\"csv\"\n","                        )"]},{"cell_type":"markdown","id":"substantial-soccer","metadata":{"id":"substantial-soccer"},"source":["You can download the CSV file and view the attributes saved.\n","\n","# Congratulations, you have compelted this exercise! Please sit patiently as the rest of attendees complete their exercises."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}